{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import seaborn as sns\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load Framingham dataset\n",
    "# Assuming the dataset is in the same directory as this script\n",
    "# You can download it from https://www.kaggle.com/datasets/amanjaiswal/framingham-dataset\n",
    "data = pd.read_csv('framingham.csv')\n",
    "\n",
    "# Quick exploration of data\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data['TenYearCHD'].value_counts())  # Target variable distribution\n",
    "\n",
    "# Step 3: Handle missing values (median imputation for simplicity)\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "# Check again for missing values\n",
    "print(\"Missing values after imputation:\\n\", data.isnull().sum())\n",
    "\n",
    "# Step 4: Feature Engineering and Preprocessing\n",
    "X = data.drop(\"TenYearCHD\", axis=1)\n",
    "y = data[\"TenYearCHD\"]\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 5: Split data into training and test sets (70%-30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Function to evaluate models quickly\n",
    "def evaluate_model(model, X_tr, y_tr, X_te, y_te):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    predictions = model.predict(X_te)\n",
    "    probas = model.predict_proba(X_te)[:, 1]\n",
    "    print(classification_report(y_te, predictions))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_te, probas))\n",
    "    cm = confusion_matrix(y_te, predictions)\n",
    "    plt.figure(figsize=(8, 6)) # check later\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()  # check later\n",
    "    plt.show()\n",
    "\n",
    "# Step 6: Define classifiers\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Step 7: Experiment groups clearly defined\n",
    "datasets = {\n",
    "    \"Original (Imbalanced)\": (X_train, y_train),\n",
    "    \"SMOTE\": SMOTE(random_state=42).fit_resample(X_train, y_train),\n",
    "    \"ADASYN\": ADASYN(random_state=42).fit_resample(X_train, y_train)\n",
    "}\n",
    "\n",
    "# Step 8: Train and evaluate each classifier on each dataset\n",
    "for dataset_name, (X_resampled, y_resampled) in datasets.items():\n",
    "    print(f\"\\n{'='*30}\\nDataset: {dataset_name}\\n{'='*30}\")\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n--- Model: {model_name} ---\")\n",
    "        evaluate_model(model, X_resampled, y_resampled, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AdvancedResearch)",
   "language": "python",
   "name": "advancedresearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
